{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "from skimage import io\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import VQA_model.VocabEncoder as VE\n",
    "import VQA_model.models.seq2vec as seq2vec\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_dataframe(json_file_path, delimiter):\n",
    "    \"\"\"\n",
    "    This function converts a JSON file to a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "    json_file_path : str : the path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    df : DataFrame : a pandas DataFrame created from the JSON file, or\n",
    "    None : if an error occurs.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Open the JSON file\n",
    "        with open(json_file_path, 'r') as json_file:\n",
    "            # Load the content of the file\n",
    "            # Assuming the JSON structure is a flat dictionary-like structure\n",
    "            # If the structure is different, this line may need adjustment\n",
    "            json_data = json.load(json_file)[delimiter]\n",
    "        \n",
    "        # Convert the JSON data to a DataFrame\n",
    "        # Note: Depending on the JSON structure, you might need a different approach\n",
    "        df = pd.DataFrame(json_data)\n",
    "\n",
    "        # Return the DataFrame\n",
    "        return df\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {json_file_path}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error occurred while decoding JSON from file: {json_file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # Catch any other exceptions that occur\n",
    "        print(f\"An unexpected error occurred: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_rows(df, delimiter):\n",
    "    \"\"\"\n",
    "    Remove rows with NaN in the 'question' column from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The modified DataFrame with rows containing NaN in 'question' column removed.\n",
    "    \"\"\"\n",
    "    # Validate if 'question' column exists in the DataFrame\n",
    "    if delimiter in df.columns:\n",
    "        # Remove rows where 'question' column is NaN\n",
    "        df_clean = df.dropna(subset=[delimiter])\n",
    "        return df_clean\n",
    "    else:\n",
    "        raise ValueError(f\"No {delimiter} column found in the DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_columns(dataframe, columns_to_remove):\n",
    "    \"\"\"\n",
    "    Remove specified columns from a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    dataframe (pd.DataFrame): The original DataFrame.\n",
    "    columns_to_remove (list): A list of column names to remove.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A new DataFrame with specified columns removed.\n",
    "    \"\"\"\n",
    "    # Check if all columns to remove are in the DataFrame\n",
    "    for col in columns_to_remove:\n",
    "        if col not in dataframe.columns:\n",
    "            raise ValueError(f\"Column '{col}' does not exist in the DataFrame.\")\n",
    "\n",
    "    # Drop the columns\n",
    "    dataframe = dataframe.drop(columns=columns_to_remove)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframes_on_column(df1, df2, common_column, how='inner'):\n",
    "    \"\"\"\n",
    "    Merge two pandas DataFrames on a specific common column.\n",
    "\n",
    "    Parameters:\n",
    "    df1 (pd.DataFrame): The first DataFrame.\n",
    "    df2 (pd.DataFrame): The second DataFrame.\n",
    "    common_column (str): The name of the common column to merge on.\n",
    "    how (str): Type of merge to be performed ('left', 'right', 'outer', 'inner'), default is 'inner'.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A new DataFrame resulting from the merge of the two input DataFrames.\n",
    "    \"\"\"\n",
    "    # Check if the common column exists in both DataFrames\n",
    "    if common_column not in df1.columns or common_column not in df2.columns:\n",
    "        raise ValueError(f\"The common column '{common_column}' must exist in both DataFrames.\")\n",
    "\n",
    "    # Merge the DataFrames on the common_column\n",
    "    result = pd.merge(df1, df2, on=common_column, how=how)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Questions\n",
    "\n",
    "PATH_questions_split_train = 'data/text/USGS_split_train_questions.json'\n",
    "PATH_questions_split_test = 'data/text/USGS_split_test_questions.json'\n",
    "PATH_questions_split_val = 'data/text/USGS_split_val_questions.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answers\n",
    "\n",
    "PATH_answers_split_train = 'data/text/USGS_split_train_answers.json'\n",
    "PATH_answers_split_test = 'data/text/USGS_split_test_answers.json'\n",
    "PATH_answers_split_val = 'data/text/USGS_split_val_answers.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All answers / questions\n",
    "\n",
    "PATH_all_questions = 'data/text/USGSquestions.json'\n",
    "PATH_all_answers = 'data/text/USGSanswers.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Questions\n",
    "\n",
    "questions_train = json_to_dataframe(PATH_questions_split_train, \"questions\")\n",
    "questions_test = json_to_dataframe(PATH_questions_split_test, \"questions\")\n",
    "questions_val = json_to_dataframe(PATH_questions_split_val, \"questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answers\n",
    "\n",
    "answers_train = json_to_dataframe(PATH_answers_split_train, \"answers\")\n",
    "answers_test = json_to_dataframe(PATH_answers_split_test, \"answers\")\n",
    "answers_val = json_to_dataframe(PATH_answers_split_val, \"answers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Remove NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Questions\n",
    "\n",
    "questions_train_nan = remove_nan_rows(questions_train, \"question\")\n",
    "questions_test_nan = remove_nan_rows(questions_test, \"question\")\n",
    "questions_val_nan = remove_nan_rows(questions_val, \"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answers\n",
    "\n",
    "answers_train_nan = remove_nan_rows(answers_train, \"answer\")\n",
    "answers_test_nan = remove_nan_rows(answers_test, \"answer\")  \n",
    "answers_val_nan = remove_nan_rows(answers_val, \"answer\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Remove unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Questions\n",
    "\n",
    "questions_train_nan_clean = remove_columns(questions_train_nan, [\"active\", \"date_added\", \"people_id\", \"answers_ids\"])\n",
    "questions_test_nan_clean = remove_columns(questions_test_nan, [\"active\", \"date_added\", \"people_id\", \"answers_ids\"])\n",
    "questions_val_nan_clean = remove_columns(questions_val_nan, [\"active\", \"date_added\", \"people_id\", \"answers_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answers\n",
    "\n",
    "answers_train_nan_clean = remove_columns(answers_train_nan, [\"active\", \"date_added\", \"people_id\", \"question_id\"])\n",
    "answers_test_nan_clean = remove_columns(answers_test_nan, [\"active\", \"date_added\", \"people_id\", \"question_id\"])\n",
    "answers_val_nan_clean = remove_columns(answers_val_nan, [\"active\", \"date_added\", \"people_id\", \"question_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Concatenating Questions & Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train\n",
    "\n",
    "train = merge_dataframes_on_column(questions_train_nan_clean, answers_train_nan_clean, \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test\n",
    "\n",
    "test = merge_dataframes_on_column(questions_test_nan_clean, answers_test_nan_clean, \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test\n",
    "\n",
    "val = merge_dataframes_on_column(questions_val_nan_clean, answers_val_nan_clean, \"id\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the VocabEncoder objects\n",
    "\n",
    "encoder_questions = VE.VocabEncoder(PATH_all_questions, questions = True)\n",
    "encoder_answers = VE.VocabEncoder(PATH_all_answers, questions = False, range_numbers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 95)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create vocabulary\n",
    "\n",
    "vocabulary_questions = encoder_questions.getVocab()\n",
    "vocabulary_answers = encoder_answers.getVocab()\n",
    "len(vocabulary_questions), len(vocabulary_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 6/929256 words are not in dictionary, thus set UNK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BayesianUniSkip(\n",
       "  (embedding): Embedding(145, 620, padding_idx=0)\n",
       "  (rnn): BayesianGRU(\n",
       "    (gru_cell): BayesianGRUCell(\n",
       "      (weight_ir): Linear(in_features=620, out_features=2400, bias=True)\n",
       "      (weight_ii): Linear(in_features=620, out_features=2400, bias=True)\n",
       "      (weight_in): Linear(in_features=620, out_features=2400, bias=True)\n",
       "      (weight_hr): Linear(in_features=2400, out_features=2400, bias=False)\n",
       "      (weight_hi): Linear(in_features=2400, out_features=2400, bias=False)\n",
       "      (weight_hn): Linear(in_features=2400, out_features=2400, bias=False)\n",
       "      (drop_ir): SequentialDropout(0.2500)\n",
       "      (drop_ii): SequentialDropout(0.2500)\n",
       "      (drop_in): SequentialDropout(0.2500)\n",
       "      (drop_hr): SequentialDropout(0.2500)\n",
       "      (drop_hi): SequentialDropout(0.2500)\n",
       "      (drop_hn): SequentialDropout(0.2500)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create the seq2vec object\n",
    "\n",
    "seq2vec = seq2vec.factory(vocabulary_questions, {'arch': 'skipthoughts', 'dir_st': 'data/skip-thoughts', 'type': 'BayesianUniSkip', 'dropout': 0.25, 'fixed_emb': False})\n",
    "for param in seq2vec.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "seq2vec.to(device)\n",
    "seq2vec.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:52<00:00,  2.02s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "BATCH_SIZE = 4000\n",
    "\n",
    "def create_batch(dataframe, start_idx, batch_size):\n",
    "    batch_data = []\n",
    "    for idx in range(start_idx, min(start_idx + batch_size, len(dataframe))):\n",
    "        row = dataframe.iloc[idx]\n",
    "        question_encoded = encoder_questions.encode(row.question)\n",
    "        answer_encoded = encoder_answers.encode(row.answer)\n",
    "\n",
    "        question_tensor = torch.tensor(question_encoded, dtype=torch.long).unsqueeze(0)  # Unsqueeze here for batch dimension\n",
    "        answer_tensor = torch.tensor(answer_encoded, dtype=torch.long)\n",
    "\n",
    "        batch_data.append((row.id, question_tensor, answer_tensor, row.type, row.img_id))\n",
    "\n",
    "    return batch_data\n",
    "\n",
    "\n",
    "def process_batch(batch_data, save_batch_accumulator, save_path, current_save_idx):\n",
    "    all_questions = torch.cat([item[1] for item in batch_data], dim=0).to(device)\n",
    "    question_representations = seq2vec(all_questions)\n",
    "\n",
    "    # Iterating over the batch to accumulate data\n",
    "    for idx, (data_id, _, answer_tensor, question_type, image_id) in enumerate(batch_data):\n",
    "        question_representation = question_representations[idx].cpu().detach()\n",
    "        answer_tensor = answer_tensor.cpu().detach()\n",
    "\n",
    "        data = {\n",
    "            'question': question_representation,\n",
    "            'answer': answer_tensor,\n",
    "            'question_type': question_type,\n",
    "            'image_id': image_id\n",
    "        }\n",
    "        save_batch_accumulator.append(data)\n",
    "\n",
    "        # Once we accumulate enough items, we save them in a batch and empty the accumulator\n",
    "        if len(save_batch_accumulator) >= BATCH_SIZE:\n",
    "            save_accumulated_data(save_batch_accumulator, save_path, current_save_idx)\n",
    "            save_batch_accumulator.clear()\n",
    "            current_save_idx += 1\n",
    "\n",
    "    return current_save_idx  # return updated save index\n",
    "\n",
    "def save_accumulated_data(data_accumulator, save_path, save_idx):\n",
    "    batch_save_path = os.path.join(save_path, f\"batch_{save_idx}.pt\")\n",
    "    torch.save(data_accumulator, batch_save_path)\n",
    "\n",
    "# Prepare a list to accumulate processed data before saving\n",
    "save_batch_accumulator = []\n",
    "current_save_idx = 0  # This keeps track of the current batch file number for saving\n",
    "\n",
    "# Directory where batch files will be saved\n",
    "save_directory = \"data/text_representations/val\"\n",
    "\n",
    "# Main loop to handle batch processing\n",
    "for start_idx in tqdm(range(0, len(val), BATCH_SIZE)):\n",
    "    batch_data = create_batch(val, start_idx, BATCH_SIZE)\n",
    "    current_save_idx = process_batch(batch_data, save_batch_accumulator, save_directory, current_save_idx)\n",
    "\n",
    "# After all batches are processed, there might be residual data that hasn't been saved yet\n",
    "if save_batch_accumulator:\n",
    "    save_accumulated_data(save_batch_accumulator, save_directory, current_save_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\damia\\dev\\RSViTQA\\textual-processing.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/damia/dev/RSViTQA/textual-processing.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test_question \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/damia/dev/RSViTQA/textual-processing.ipynb#X46sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test_answer \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/damia/dev/RSViTQA/textual-processing.ipynb#X46sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m test_question_encoded \u001b[39m=\u001b[39m encoder_questions\u001b[39m.\u001b[39mencode(test_question)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "test_question = train.iloc[0][\"question\"]\n",
    "test_answer = train.iloc[0][\"answer\"]\n",
    "\n",
    "test_question_encoded = encoder_questions.encode(test_question)\n",
    "test_answer_encoded = encoder_answers.encode(test_answer)\n",
    "\n",
    "test_question_encoded_numpy = np.array(test_question_encoded, dtype=\"int16\")\n",
    "question_tensor = torch.tensor(test_question_encoded_numpy, dtype=torch.long, device=device).unsqueeze(1).to(device)\n",
    "\n",
    "representation = seq2vec(question_tensor)\n",
    "\n",
    "question_tensor.shape\n",
    "\n",
    "linear_q = nn.Linear(2400, 1200)\n",
    "x_q = linear_q(representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## R\n",
    "visual_representation = torch.load(\"/Users/kaanaydin/Documents/02 School/02 Master's degree/03 HS23/02 Deep Learning/deep-learning/data-representations/visual/0.pt\")\n",
    "\n",
    "## Building visual head\n",
    "output_size = (512 / 32)**2\n",
    "visual = torch.nn.Conv2d(2048,int(2048/output_size), 1)\n",
    "# visual.to(device=device);\n",
    "\n",
    "##  Next visual step\n",
    "visual_next = visual(visual_representation).view(-1, 2048)\n",
    "\n",
    "## To fusion dimensions\n",
    "visual_linear = nn.Linear(2048, 1200)\n",
    "x_v = visual_linear(visual_next)\n",
    "x_v = nn.Tanh()(x_v)\n",
    "x_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_classif1 = nn.Linear(1200, 256)\n",
    "linear_classif2 = nn.Linear(256, 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.mul(x_v, x_q)\n",
    "x = linear_classif1(x)\n",
    "x = linear_classif2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 2400])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1200])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "text = torch.load(os.path.join(\"data/text_representations/train/\", str(1200) + \".pt\"))\n",
    "question = text[\"question\"]\n",
    "print(question.shape)\n",
    "\n",
    "img = torch.load(os.path.join(\"data/image_representations/\", str(1200) + \".pt\"))\n",
    "img.shape\n",
    "\n",
    "## Building visual head\n",
    "output_size = (512 / 32)**2\n",
    "visual = torch.nn.Conv2d(2048,int(2048/output_size), 1)\n",
    "# visual.to(device=device);\n",
    "\n",
    "##  Next visual step\n",
    "visual_next = visual(img).view(-1, 2048)\n",
    "\n",
    "## To fusion dimensions\n",
    "visual_linear = nn.Linear(2048, 1200)\n",
    "x_v = visual_linear(visual_next)\n",
    "x_v = nn.Tanh()(x_v)\n",
    "x_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1200])\n"
     ]
    }
   ],
   "source": [
    "linear_q = nn.Linear(2400, 1200)\n",
    "x_q = linear_q(question)\n",
    "x_q = nn.Tanh()(x_q)\n",
    "print(x_q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1200])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.mul(x_v, x_q)\n",
    "x.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
